"""
Phase 1: ì§ˆë¬¸ ìƒì„± ë¼ìš°í„°
ìê¸°ì†Œê°œì„œ ì—…ë¡œë“œ â†’ Agent í˜¸ì¶œ â†’ ì§ˆë¬¸ ìƒì„±
"""

import os
import json
import asyncio
import logging
from datetime import datetime
from typing import Optional

import vertexai
from fastapi import APIRouter, HTTPException, UploadFile, File
from pydantic import BaseModel
from google.cloud import storage

logger = logging.getLogger(__name__)

# í™˜ê²½ ì„¤ì • - í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ (ê¸°ë³¸ê°’ ì—†ìŒ)
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
QUESTION_AGENT_ID = os.getenv("QUESTION_AGENT_ID")

if not all([PROJECT_ID, BUCKET_NAME, QUESTION_AGENT_ID]):
    raise ValueError(
        "í™˜ê²½ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”: "
        "GOOGLE_CLOUD_PROJECT, GCS_BUCKET_NAME, QUESTION_AGENT_ID"
    )

router = APIRouter(prefix="/api", tags=["ì§ˆë¬¸ ìƒì„±"])


# ========== Response ëª¨ë¸ ==========

class GenerateQuestionsResponse(BaseModel):
    """ì§ˆë¬¸ ìƒì„± ì‘ë‹µ"""
    status: str
    message: str
    sessionId: Optional[str] = None  # âœ… ì„¸ì…˜ ID ì¶”ê°€
    question_count: Optional[int] = None
    gcs_uri: Optional[str] = None
    company_name: Optional[str] = None
    pdf_path: Optional[str] = None
    timestamp: Optional[str] = None


# ========== ì—”ë“œí¬ì¸íŠ¸ ==========

@router.post("/generate-questions", response_model=GenerateQuestionsResponse)
async def generate_questions(resume_file: UploadFile = File(...)):
    """
    ìê¸°ì†Œê°œì„œ PDF ì—…ë¡œë“œ ë° ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    
    **Process:**
    1. PDF íŒŒì¼ ê²€ì¦
    2. GCS ë²„í‚·ì— ì €ì¥ (pdf/)
    3. Pre-signed URL ìƒì„±
    4. ë°°í¬ëœ ì§ˆë¬¸ ìƒì„± Agent í˜¸ì¶œ
    5. ì§ˆë¬¸ ìƒì„± ëŒ€ê¸° (ë¹„ë™ê¸°)
    6. ê²°ê³¼ ë°˜í™˜
    
    **Returns:**
    - status: "success" | "error"
    - message: ê²°ê³¼ ë©”ì‹œì§€
    - question_count: ìƒì„±ëœ ì§ˆë¬¸ ê°œìˆ˜
    - company_name: ì§€ì› ê¸°ì—…ëª…
    - gcs_uri: GCS ì €ì¥ ê²½ë¡œ
    """
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ“¤ ì§ˆë¬¸ ìƒì„± ìš”ì²­ ìˆ˜ì‹ ")
        
        # 1. PDF ê²€ì¦
        if not resume_file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        logger.info(f"ğŸ“„ íŒŒì¼ëª…: {resume_file.filename}")
        
        # âœ… ì„¸ì…˜ ID ìƒì„± (ìµœìš°ì„ !)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = f"session_{timestamp_str}"
        logger.info(f"ğŸ†” ì„¸ì…˜ ID ìƒì„±: {session_id}")
        
        # 2. GCSì— PDF ì €ì¥
        storage_client = storage.Client(project=PROJECT_ID)
        bucket = storage_client.bucket(BUCKET_NAME)
        
        pdf_filename = f"{session_id}_resume.pdf"  # â† ì„¸ì…˜ ID í¬í•¨
        pdf_path = f"pdf/{pdf_filename}"
        
        logger.info(f"â¬†ï¸ GCS ì—…ë¡œë“œ: {pdf_path}")
        
        blob = bucket.blob(pdf_path)
        pdf_content = await resume_file.read()
        blob.upload_from_string(pdf_content, content_type="application/pdf")
        
        logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ ({len(pdf_content):,} bytes)")
        
        # 3. GCS URI ìƒì„± (Presigned URL ëŒ€ì‹  gs:// ì§ì ‘ ì‚¬ìš©)
        gcs_uri = f"gs://{BUCKET_NAME}/{pdf_path}"
        logger.info(f"ğŸ“¦ GCS URI ìƒì„±: {gcs_uri}")
        
        # 4. Agent í˜¸ì¶œ
        logger.info(f"ğŸ¤– ì§ˆë¬¸ ìƒì„± Agent í˜¸ì¶œ...")
        
        client = vertexai.Client(project=PROJECT_ID, location=LOCATION)
        # í”„ë¡œì íŠ¸ IDë¥¼ ì‚¬ìš© (í”„ë¡œì íŠ¸ ë„˜ë²„ëŠ” ìë™ìœ¼ë¡œ í•´ì„ë¨)
        agent_resource_name = f"projects/{PROJECT_ID}/locations/{LOCATION}/reasoningEngines/{QUESTION_AGENT_ID}"
        adk_app = client.agent_engines.get(name=agent_resource_name)
        
        # âœ… ì„¸ì…˜ ID í¬í•¨ ë©”ì‹œì§€ (gs:// URI ì „ë‹¬)
        user_message = f"""ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œì„œ PDFë¥¼ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.

[SESSION_ID: {session_id}]

GCS URI: {gcs_uri}

ì´ ìê¸°ì†Œê°œì„œë¥¼ ë¶„ì„í•˜ê³ , ì§€ì› ê¸°ì—…ì„ íŒŒì•…í•œ í›„, 
í•´ë‹¹ ê¸°ì—… ì •ë³´ë¥¼ ì›¹ ê²€ìƒ‰í•˜ì—¬ ë©´ì ‘ ë¶„ì„ ë°ì´í„°ë¥¼ GCSì— ì €ì¥í•´ì£¼ì„¸ìš”.

ìë™ìœ¼ë¡œ ëª¨ë“  ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ê³  GCSì— ì €ì¥ê¹Œì§€ ì™„ë£Œí•´ì£¼ì„¸ìš”!"""
        
        # 5. Agent ì‹¤í–‰
        events = []
        async for event in adk_app.async_stream_query(
            user_id="web_user",
            message=user_message
        ):
            events.append(event)
        
        logger.info(f"âœ… Agent ì‹¤í–‰ ì™„ë£Œ ({len(events)}ê°œ ì´ë²¤íŠ¸)")
        
        # 6. Agent ì‘ë‹µ í™•ì¸
        if events:
            last_event = events[-1]
            logger.info(f"ğŸ“¦ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸: {type(last_event).__name__}")
            if hasattr(last_event, 'content'):
                logger.info(f"ğŸ“ Agent ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {str(last_event)[:500]}...")
        
        # 7. GCS ì €ì¥ ëŒ€ê¸° (10ì´ˆë¡œ ì¦ê°€)
        logger.info("â³ GCS ì €ì¥ ëŒ€ê¸° ì¤‘ (10ì´ˆ)...")
        await asyncio.sleep(10)
        
        # 8. âœ… ìƒì„±ëœ ë¶„ì„ íŒŒì¼ í™•ì¸ (ì„¸ì…˜ ID ê¸°ë°˜)
        analysis_filename = f"{session_id}_analysis.json"
        analysis_path = f"interview_questions/{analysis_filename}"
        
        logger.info(f"ğŸ” ë¶„ì„ íŒŒì¼ ê²€ìƒ‰: {analysis_path}")
        
        analysis_blob = bucket.blob(analysis_path)
        
        if not analysis_blob.exists():
            error_detail = (
                f"ë¶„ì„ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {analysis_filename}. "
                f"Agent ì´ë²¤íŠ¸: {len(events)}ê°œ. "
                f"Agentê°€ save_resume_analysis ë° update_company_info í•¨ìˆ˜ë¥¼ í˜¸ì¶œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            )
            logger.error(f"âŒ {error_detail}")
            
            # Agent ì´ë²¤íŠ¸ ìƒì„¸ ì¶œë ¥
            for i, event in enumerate(events[-3:], start=len(events)-2):
                logger.error(f"  Event [{i}]: {str(event)[:200]}")
            
            raise HTTPException(
                status_code=500,
                detail=error_detail
            )
        
        logger.info(f"ğŸ“‚ ë¶„ì„ íŒŒì¼ í™•ì¸: {analysis_blob.name} (í¬ê¸°: {analysis_blob.size} bytes)")
        
        # íŒŒì¼ ë‚´ìš© ë¡œë“œ
        analysis_text = analysis_blob.download_as_text()
        logger.info(f"ğŸ“„ íŒŒì¼ ë‚´ìš© ê¸¸ì´: {len(analysis_text)} chars")
        
        if not analysis_text.strip():
            raise HTTPException(
                status_code=500,
                detail="ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Agent ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )
        
        analysis_data = json.loads(analysis_text)
        
        company_name = analysis_data.get("company_name", "Unknown")
        # âœ… ë¶„ì„ íŒŒì¼ì´ë¯€ë¡œ question_countëŠ” ì—†ìŒ (ë‚˜ì¤‘ì— session_agentì—ì„œ ì§ˆë¬¸ ìƒì„±)
        
        logger.info(f"âœ… ìê¸°ì†Œê°œì„œ ë° ê¸°ì—… ë¶„ì„ ì™„ë£Œ: {company_name}")
        logger.info(f"   ì„¸ì…˜ ID: {session_id}")
        logger.info("=" * 60)
        
        return GenerateQuestionsResponse(
            status="success",
            message=f"ìê¸°ì†Œê°œì„œ ë° ê¸°ì—… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            sessionId=session_id,  # âœ… ì„¸ì…˜ ID ì¶”ê°€
            question_count=None,  # ë¶„ì„ ë‹¨ê³„ì—ì„œëŠ” ì§ˆë¬¸ ìˆ˜ ì—†ìŒ
            gcs_uri=f"gs://{BUCKET_NAME}/{analysis_blob.name}",
            company_name=company_name,
            pdf_path=pdf_path,
            timestamp=datetime.now().isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
